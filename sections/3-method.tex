\chapter{Methods}

This chapter introduces the outline of this project, the datasets used in the experiments, the hardware and software information, the detailed 
explanation of the experiment design and the construction of deep learning models.

\section{Projects Outline}
The project was performed in following steps:
\begin{itemize}
    \item Access and preprocess the datasets if necessary.
    \item Apply rCOMMIT experiment on the target datasets
    \item Extract pseudo ground truth from the experiment results.
    \item Construct deep learning models.
    \item Train and tune the models to achieve stable performance.
    \item Compare the results between models.
    \item Compare the results with previous research.
  \end{itemize}

\section{Datasets}
In this project, the preprocessed datasets of six different subjects from the young adult data set of the Human Connectome Project \cite{vanessenWUMinnHumanConnectome2013} are used as raw data.
The tractograms are generated by the authors of \cite{TractSegFastAccurate} with iFOD2 \cite{tournierImprovedProbabilisticStreamlinesa}. 
Ten million streamlines are generated for each subject and the length of streamlines are restricted to be in the range between 40 mm to 250 mm.
The tracking was further constrained by anatomical priors based on the segmentation of different tissue types in the brain \cite{smithAnatomicallyconstrainedTractographyImproved2012}.
And the ten million streamlines cover the whole white matter of each subject. 

\section{Hardware and Software}

Two Nvidia GPUs are used for the training of deep learning models in this project, which respectively are of 4GB and 11GB memory. 
The latter GPU is deployed on a platform, Medical Artificial Intelligence Aggregator (MAIA) \cite{MAIA}, which aims to integrate and promote collaborations in the medical field.

COMMIT \cite{daducciCOMMITConvexOptimization2015} is used as the main filtering tool in the experiments. 
SIFT \cite{smithSIFTSphericaldeconvolutionInformed2013} is used in the previous research from which the results are compared in this study.
MRtrix3 \cite{tournierMRtrix3FastFlexible2019} helps visualize the data to check the white matter region is covered by the streamlines. 
The type of raw data is originally in tck file, and a script from ScilPy realizes the conversion between tck and trk files.
Python library Dipy takes care of postprocessing and analysing the tractograms in the training with DL models.
The neural network is built based on TensorFlow, and the monitor of the training process is done with TensorBoard.

\section{Randomized COMMIT (rCOMMIT)}
Lack of ground truth of tractogram has always been a challenge in this field. For filtering methods, there is no ground truth for the results either. 
Avoiding biases in the results and extracting plausible information from the filtering methods, randomized COMMIT is purposed. 
The main idea of this method is to use multiple assessments from COMMIT on the same streamline in different subsets in order to classify streamlines into different groups.
The filtering process can be regarded as a binary classification, since in one time running of COMMIT, the streamline is either accepted or rejected by the filtering method.
The results from multiple times of filtering are seen as votes that are either positive or negative.

Another concern is the bias of the size of the dataset. Usually for a tractogram of 10 million streamlines, around 2\% - 2.5\% streamlines are remained after filtering.
During the filtering, when the remaining streamlines are too small to guarantee the stability of the computation, the filtering method would terminate the process.
To investigate the performance of COMMIT in different sizes, 
multiple subsets are generated from the raw data. From the raw dateset to the smallest dateset, the size of subsets is automatically chosen, by halving
the subset size. And the smallest size is set to be 2.5\% of the input size. Therefore, all the used sizes include $SS= \left \{  1\times 10^7, 5\times 10^6, 2.5\times 10^6, 1.25\times 10^6, 6.25\times 10^5, 5\times 10^5, 2.5\times 10^5 \right \}$ 

Fig \ref{fig:pipe} shows the whole pipeline of rCOMMIT and the pseudo algorithm of the pipeline is shown in \ref{fig:algo}. 
The process of rCOMMIT will be explained more in details with the pseudo algorithm. rCOMMIT will run on all subsets with predefined
size $n \in SS$, and for each $n$ the repetition time $k$ is calculated according to the formula below:
\begin{gather}\label{computek}  
    k = \tau M/n
\end{gather}
$M$ is the original number of streamlines in the tractogram, and $\tau$ is the parameter set in the experiments, 
which indicates the purposed average filtering times by COMMIT for the streamlines. This parameter helps achieve enough votes for classifying streamlines into groups.
For example, with $\tau=5$, the repetition time $k \in \left \{5, 10, 20, 40, 80, 100, 200 \right \}$ for each size in $SS$.
Therefore, with multiple times of filtering, the acceptance rate (AR) of each streamline can be obtained, which is computed below:
\begin{gather}\label{AR}
    AR(s) = \frac{P(s)}{P(s)+ N(s)}
\end{gather}
$AR(s)$ refers to the acceptance rate for the streamline $s$ after receiving all the votes from the experiment in different sizes.
$P(s)$ stands for the positive votes while $N(s)$ are the negative votes.
 The number of 
It's worthy mentioning that each subset is randomly sampled from the raw data. 
Although a target average number of filtering is set, but the number of occurrences of each streamline is not ensured, 
which means the numbers of votes for the streamlines can be various, or even zero.




\begin{figure}[ht]
    \centering
    \includegraphics[width= 16cm]{figures/pipe.jpg}
        \caption{The pipeline of rCOMMIT. 
        }
    \label{fig:pipe}
\end{figure}


\begin{algorithm}
    \caption{Pseudocode of rCOMMIT. For the chosen subset size $n$, $k$ random subsets of the tractogram are randomly extracted and filtered with COMMIT to 
    receive the index of the accepted and rejected streamlines $subset_i^P$ and $subset_i^N$. These are used to update the number of votes and compute the acceptance rate in the end.}
    
    \KwIn{
    \begin{tabular}{ll}
    $T = \{t_1, t_2, \ldots, t_{M}\}$ : tractogram,\\
    $SS = \{s_1, s_2, \ldots, s_N\}$ : subset sizes
    \end{tabular}}
    
    \KwOut{
    \begin{tabular}{ll}
    $P = \{p_1, p_2, \ldots, p_M\}$: accepted times,\\
    $N = \{n_1, n_2, \ldots, n_M\}$: rejected times
    \end{tabular}}
    
    \BlankLine
    \BlankLine
    %\Begin{
    Initialize $P$ and $N$ with zeros\\
    \ForAll{$n \in SS$}{
    Initialize $P_n$ and $N_n$ with zeros\\
    $k \gets \tau M/n$\\
    \For{$i \in \{1, \ldots, k\}$}{ 
    $subset_i \gets \{r_1, \ldots,r_n\} \subseteq T$, where $ r_{1, \ldots, n}$ are randomly selected from $T$\\
    $[subset_{i}^P, subset_{i}^N] \gets COMMIT(subset_i)$\\
    $P_n(subset_i^P) \gets P_n(subset_i^P)+1$\\
    $N_n(subset_i^N) \gets N_n(subset_i^N)+1$
    }
    $P= P+P_n$\\
    $N= N+N_n$\\
    
    }
    \label{fig:algo}
    
\end{algorithm}


\section{Pseudo Ground Truth}

\section{Deep Learning Model Construction}





